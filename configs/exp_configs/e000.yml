description: "default config"
runner: r002
SINGLE_FOLD: false
split:
    split_type: skf
    split_num: 5
    shuffle: true
    random_state: 71
    abhishek5: ./inputs/datasets/abhishek_folds/train_folds.csv
    abhishek8: ./inputs/datasets/abhishek_folds/train_8folds.csv
loader:
    trn_sampler_type: random
    trn_batch_size: 96
    tst_sampler_type: sequential
    tst_batch_size: 96
    dataset_type: tse_headtail_dataset
    neutral_weight: 1.0
    longer_posneg_rate: 1.0
dataset:
    tokenizer_type: roberta_bytelevel_bpe
    pretrained_model_name_or_path: ./inputs/datasets/
    do_lower_case: true
    add_pair_prefix_space: false
    max_length: 125
    tokenize_period: false
    tail_index: natural
    use_magic: false
model:
    model_type: roberta-headtail
    pretrained_model_name_or_path: roberta-base
    num_output_units: 125
fobj:
    fobj_type: ce
fobj_index_diff:
    fobj_type: nothing
fobj_segmentation:
    fobj_type: nothing
optimizer:
    optim_type: adam
    lr: 0.00003
scheduler:
    scheduler_type: cosine
    every_step_unit: 0.2
    cosine_eta_min: 0.000001
    multistep_milestones:
        - 1
        - 3
    multistep_gamma: 0.2
train:
    max_epoch: 5
    warmup_epoch: 1
    thresh_unit: 0.05  # no meaning for head tail
    rm_neutral: false
    warmup_batch: 0
    ema_mu: 0.9
    ema_level: batch
    ema_n: 0  # < 1 means no ema
    accum_mod: 1
    use_special_mask: false
    use_offsets: false
    head_ratio: 1
    tail_ratio: 1
    segmentation_loss_ratios: 1
    loss_weight_type: nothing
    pseudo:
    use_dist_loss: false
    single_word: false
predict:
    neutral_origin: true
    head_tail_equal_handle: tail
    pospro:
        head_tail_1: false
        req_shorten: false
        regex_1: false
        regex_2: false
        regex_3: false
        magic: false
        magic_2: false
    use_offsets: false
    tail_index: natural
    single_word: false
invalid_labels:
    # - ./inputs/nes_info/invalid_labels.csv
